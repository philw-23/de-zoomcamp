{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c6bd65-62c2-4624-8c8d-805b8489ecdc",
   "metadata": {},
   "source": [
    "# Loading Data to Local location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "445bda46-ce2e-46ec-9d6c-494d4013e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages needed for data write\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a56534f8-aeaf-4b36-8c58-da87caa2c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define URLs for data pulls\n",
    "url_taxi = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet'\n",
    "url_location = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9f51c28-8cca-4dda-ae88-8848ffdaae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output files for data\n",
    "file_taxi = './jan_2023_yellow_tripdata.parquet'\n",
    "file_location = './nyc_taxi_location_zones.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4fa3128-1533-4e87-9487-6b9229df666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define function for writing url data to local file\n",
    "    * url is the url where data is pulled from\n",
    "    * f_name is the location where the data will be written\n",
    "'''\n",
    "def write_data_local(url, f_name):\n",
    "    if not os.path.isfile(f_name): # Check if file already exists!\n",
    "        response = requests.get(url, stream=True) # Stream file with requests to avoid loading all in memory\n",
    "        if response.status_code == 200: # Only continue if we have a successful request\n",
    "            with open(f_name, 'wb') as out: # Open a file to write results to\n",
    "                for resp in response.iter_content(chunk_size=500000): # Stream in 5mb chunks\n",
    "                    if resp: # Handle end cases\n",
    "                        out.write(resp)\n",
    "            print('Data successfully written!')\n",
    "        else:\n",
    "            print(f'Error Encountered! {response.status_code}')\n",
    "    else:\n",
    "        print('File already exists - no need to rewrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd43b3dd-eead-4dce-ac77-06e34d57b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute for taxi data\n",
    "write_data_local(url_taxi, file_taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8702f37c-0c24-421d-8f6f-d902e2bf3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute for location data\n",
    "write_data_local(url_location, file_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e491cc-2325-4d7b-9e5e-eed374ef9e82",
   "metadata": {},
   "source": [
    "# Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcb68dea-4da2-43f2-ad63-8a02c7e44de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for data exploration\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2841255-6c61-4826-9334-4b10014800b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View number of rows in parquet dataset\n",
    "# Because the file is large we do this in chunks to avoid loading everything to memory\n",
    "taxi_pq = pq.ParquetFile(file_taxi) # Open the file\n",
    "num_rows = 0\n",
    "for i in range(taxi_pq.num_row_groups): # iterate over all row groups\n",
    "    row_group = taxi_pq.read_row_group(i) # Load in specific row group\n",
    "    num_rows += row_group.num_rows # Add the number of rows\n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1679eb5-87a1-494d-9db9-6f37716fb5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also view the schema information on the parquet file!\n",
    "taxi_data = next(taxi_pq.iter_batches(batch_size=100000))\n",
    "taxi_data.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53855a1c-6f9f-433a-84b1-bccf281ab497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas and check data\n",
    "taxi_df = taxi_data.to_pandas() # Converts parquet table to pandas\n",
    "taxi_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c22865-36ea-4b69-a697-453792a4e1c0",
   "metadata": {},
   "source": [
    "# Writing data to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6de0ad28-de21-4be8-8a94-0303e9c6e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages needed\n",
    "import time # For timing purposes!\n",
    "from sqlalchemy import URL, create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50cd40e2-0135-4978-bc99-048537b20e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URL and engine object\n",
    "pg_URL = URL.create(\n",
    "    drivername='postgresql', # Driver to use\n",
    "    username='root', # Username of login\n",
    "    password='root', # Password of login\n",
    "    host='localhost', # Host for login\n",
    "    port='5432', # Connection port\n",
    "    database='ny_taxi' # Database for connection\n",
    ")\n",
    "# Note: if the below fails, you may need to install psycopg2-binary via pip in your virtual env\n",
    "pg_engine = create_engine(pg_URL) # Create URL (NOTE: postgres container must be active for this to work!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf00ce1-0cf8-4631-b71c-dbcf02dd1396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View create table statement that will be ran\n",
    "table_name = 'yellow_taxi_data' # Name to use for the table in the database\n",
    "print(pd.io.sql.get_schema(taxi_df, name=table_name, con=pg_engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f83128-14f4-4e1c-9358-97c05df6694d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Iterate through chunks to write data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m init_time \u001b[38;5;241m=\u001b[39m \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# Total start time\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Iterate through in 100K sized batches as file is too large for memory\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pg_engine\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;28;01mas\u001b[39;00m conn:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "# Iterate through chunks to write data\n",
    "init_time = time.time() # Total start time\n",
    "\n",
    "# Iterate through in 100K sized batches as file is too large for memory\n",
    "with pg_engine.connect() as conn:\n",
    "    for idx, batch in enumerate(taxi_pq.iter_batches(batch_size=100000)): \n",
    "        # idx is the index of the batch, batch is the data to write\n",
    "        batch_df = batch.to_pandas() # Convert to pandas df\n",
    "        if idx == 0: # On first batch, create table and replace if it exists\n",
    "            batch_df.head(0).to_sql(name=table_name, con=conn, \n",
    "                                    if_exists='replace', index=False) # Write only header, clear data present\n",
    "        print(f'Writing batch {str(idx + 1)}')\n",
    "        start_time = time.time() # Start time of write\n",
    "        batch_df.to_sql(name=table_name, con=conn, if_exists='append',) # Write batch and append to result\n",
    "        total_time = time.time() - start_time # Time to write batch\n",
    "        print(f'Batch {str(idx + 1)} completed in {str(round(total_time, 3))} seconds')\n",
    "    \n",
    "    # Write out total completion time\n",
    "    print(f'Completed write in {str(round(time.time() - init_time, 3))} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "34684604-f7ea-4ca3-af6c-d7bfac9f7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The other file is much smaller and can be handled with pure pandas\n",
    "taxi_zones = pd.read_csv(file_location)\n",
    "with pg_engine.connect() as conn:\n",
    "    taxi_zones.to_sql(name='pickup_locations', con=conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "de24cfda-d81d-42c0-a122-6f53825b42e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close engine once completed\n",
    "pg_engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
